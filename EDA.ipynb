{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `walmart_stock.csv` file as our dataset to analyse the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Create an Apache Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Load the `walmart_stock.csv` file into a dataframe and infer the data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util = spark.read.csv(\"walmart_stock.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Display the column names and print the dataframe schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Print out the first five rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06 00:00:00|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09 00:00:00|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Use `describe()` method to get statistical information on the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Use `format_number` function to format the numbers for just showing up to two decimal places. \n",
    "[format_number() documentation](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=format_number#pyspark.sql.functions.format_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df_util_formatted = df_util.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+--------+--------+\n",
      "|summary|    Open|    High|     Low|   Close|  Volume|\n",
      "+-------+--------+--------+--------+--------+--------+\n",
      "|  count|1,258.00|1,258.00|1,258.00|1,258.00|    1258|\n",
      "|   mean|   72.36|   72.84|   71.92|   72.39| 8222093|\n",
      "| stddev|    6.77|    6.77|    6.74|    6.76| 4519780|\n",
      "|    min|   56.39|   57.06|   56.30|   56.42| 2094900|\n",
      "|    max|   90.80|   90.97|   89.25|   90.47|80898100|\n",
      "+-------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util_formatted.select(df_util_formatted['summary'],\n",
    "                 format_number(df_util_formatted['Open'].cast('float'), 2). alias('Open'),\n",
    "                  format_number(df_util_formatted['High'].cast('float'), 2). alias('High'),\n",
    "                  format_number(df_util_formatted['Low'].cast('float'), 2). alias('Low'),\n",
    "                  format_number(df_util_formatted['Close'].cast('float'), 2). alias('Close'),\n",
    "                  df_util_formatted['Volume'].cast('int'). alias('Volume')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util_formatted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7- Create a new coulmn called HV Ratio on a new dataframe that returns the ratio of the High Price versus volume of stock traded for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "avg_HV_ratio = df_util.agg(avg(df_util['High'] / df_util['Volume']).alias('Avg HV Ratio'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        Avg HV Ratio|\n",
      "+--------------------+\n",
      "|1.058087386431558E-5|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_HV_ratio.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_ratio_df = df_util.withColumn(\"HV Ratio\", df_util['High'] / df_util['Volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of DataFrame[Date: timestamp, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double, HV Ratio: double]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hv_ratio_df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8- What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|               Date|max(High)|\n",
      "+-------------------+---------+\n",
      "|2016-12-30 00:00:00|    69.43|\n",
      "|2016-12-29 00:00:00|69.519997|\n",
      "|2016-12-28 00:00:00|     70.0|\n",
      "|2016-12-27 00:00:00|    69.82|\n",
      "|2016-12-23 00:00:00|    69.75|\n",
      "|2016-12-22 00:00:00|71.239998|\n",
      "|2016-12-21 00:00:00|     72.0|\n",
      "|2016-12-20 00:00:00|    71.93|\n",
      "|2016-12-19 00:00:00|    71.75|\n",
      "|2016-12-16 00:00:00|71.639999|\n",
      "|2016-12-15 00:00:00|71.800003|\n",
      "|2016-12-14 00:00:00|72.480003|\n",
      "|2016-12-13 00:00:00|72.230003|\n",
      "|2016-12-12 00:00:00|71.779999|\n",
      "|2016-12-09 00:00:00|    70.43|\n",
      "|2016-12-08 00:00:00|70.900002|\n",
      "|2016-12-07 00:00:00|70.650002|\n",
      "|2016-12-06 00:00:00|70.389999|\n",
      "|2016-12-05 00:00:00|70.989998|\n",
      "|2016-12-02 00:00:00|70.949997|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.groupBy('Date').agg({'High':'max'}).orderBy('Date', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|               Date|max(High)|\n",
      "+-------------------+---------+\n",
      "|2016-12-30 00:00:00|    69.43|\n",
      "|2016-12-29 00:00:00|69.519997|\n",
      "|2016-12-28 00:00:00|     70.0|\n",
      "|2016-12-27 00:00:00|    69.82|\n",
      "|2016-12-23 00:00:00|    69.75|\n",
      "|2016-12-22 00:00:00|71.239998|\n",
      "|2016-12-21 00:00:00|     72.0|\n",
      "|2016-12-20 00:00:00|    71.93|\n",
      "|2016-12-19 00:00:00|    71.75|\n",
      "|2016-12-16 00:00:00|71.639999|\n",
      "|2016-12-15 00:00:00|71.800003|\n",
      "|2016-12-14 00:00:00|72.480003|\n",
      "|2016-12-13 00:00:00|72.230003|\n",
      "|2016-12-12 00:00:00|71.779999|\n",
      "|2016-12-09 00:00:00|    70.43|\n",
      "|2016-12-08 00:00:00|70.900002|\n",
      "|2016-12-07 00:00:00|70.650002|\n",
      "|2016-12-06 00:00:00|70.389999|\n",
      "|2016-12-05 00:00:00|70.989998|\n",
      "|2016-12-02 00:00:00|70.949997|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.groupBy('Date').agg({'High':'max'}).orderBy('Date', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2015, 1, 13, 0, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.orderBy(df_util['High'].desc()).head()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9-What is the mean of the Close column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.agg({'Close':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10- How many days was the Close lower than 70 USD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_util.createOrReplaceTempView(\"walmart_stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|num_of_days_less_70|\n",
      "+-------------------+\n",
      "|                397|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) as num_of_days_less_70\\\n",
    "            FROM walmart_stock WHERE Close <70 LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of days where the Close was lower than 70 USD is 397\n"
     ]
    }
   ],
   "source": [
    "num_days_close_lt_70 = spark.sql(\"SELECT COUNT(*) FROM walmart_stock WHERE Close < 70\").collect()[0][0]\n",
    "print(f\"The number of days where the Close was lower than 70 USD is {num_days_close_lt_70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.filter(df_util['Close']<70).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11-What percentage of the time was the High greater than 80 USD ?\n",
    "#### In other words, (Number of High Days>80)/(Total Days in the dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|  high_percentage|\n",
      "+-----------------+\n",
      "|9.141494435612083|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT (COUNT(*) / (SELECT COUNT(*) FROM walmart_stock)) * 100 AS high_percentage\\\n",
    "          FROM walmart_stock\\\n",
    "          WHERE High > 80\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of time when the High was greater than 80 USD is: 9.14%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "# Count the number of days when the High was greater than 80\n",
    "high_count = df_util.filter(df_util['High'] > 80).count()\n",
    "\n",
    "# Calculate the total number of days in the dataframe\n",
    "total_count = df_util.count()\n",
    "\n",
    "# Calculate the percentage of the time when the High was greater than 80\n",
    "percentage_high = (high_count/total_count) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"The percentage of time when the High was greater than 80 USD is: {round(percentage_high, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12-What is the correlation between High and Volume?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_util.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: timestamp, Open: double, High: double, Low: double, Close: double, Volume: int, Adj Close: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_util.withColumn(\"High\",df_util.High.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3384326061737161"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "format_number(df_util['Close'].cast('float'), 2).alias('High')\n",
    "df_util['Volume'].cast('int').alias('Volume')\n",
    "\n",
    "df_util.stat.corr('High', 'Volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|High_formatted|Volume_formatted|\n",
      "+--------------+----------------+\n",
      "|         61.06|            null|\n",
      "|         60.35|            null|\n",
      "|         59.62|            null|\n",
      "|         59.45|            null|\n",
      "|         59.55|            null|\n",
      "+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "\n",
    "df_util_formatted = df_util.withColumn('Open_formatted', format_number(df_util['Open'], 2).cast('double')) \\\n",
    "                           .withColumn('High_formatted', format_number(df_util['High'], 2).cast('double')) \\\n",
    "                           .withColumn('Low_formatted', format_number(df_util['Low'], 2).cast('double')) \\\n",
    "                           .withColumn('Close_formatted', format_number(df_util['Close'], 2).cast('double')) \\\n",
    "                           .withColumn('Volume_formatted', format_number(df_util['Volume'], 0).cast('integer')) \\\n",
    "                           .withColumn('Adj_Close_formatted', format_number(df_util['Adj Close'], 2).cast('double'))\n",
    "\n",
    "df_util_formatted.select('High_formatted', 'Volume_formatted').show(5)\n",
    "\n",
    "df_util_formatted.stat.corr('High_formatted', 'Volume_formatted')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13- What is the max High per year (use GroupBy)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year = df_util.withColumn('Year', year(df_util['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|year|max(High)|\n",
      "+----+---------+\n",
      "|2015|90.970001|\n",
      "|2013|81.370003|\n",
      "|2014|88.089996|\n",
      "|2012|77.599998|\n",
      "|2016|75.190002|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year.groupBy('year').max('High').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14- What is the average Close for each Calendar Month (close price for Jan,Feb, Mar, etc)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df = df_util.withColumn('Month', month(df_util['Date']))\n",
    "month_avg = month_df.select('Month', \"Close\").groupBy('Month').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|       avg(Close)|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "month_avg.select('Month', 'avg(Close)').orderBy('Month').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3995aea5be1ae82dd11e794b47fde2cc31ea32f6b130994171a5467e983bbaf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
